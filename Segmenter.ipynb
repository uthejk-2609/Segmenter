{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TLq4yqajLdS",
        "outputId": "09605970-3852-40d0-9e39-8b02d7a312ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYHceQzNgE1m"
      },
      "outputs": [],
      "source": [
        "from einops.layers.torch import Rearrange, Reduce\n",
        "from torchsummary import summary\n",
        "import torch.nn.functional as F\n",
        "from einops import repeat\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as tt\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.datasets import ImageFolder\n",
        "import os\n",
        "import torchvision\n",
        "import torch\n",
        "\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768, img_size=256):\n",
        "        self.patch_size = patch_size\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, emb_size,\n",
        "                      kernel_size=patch_size, stride=patch_size),\n",
        "            Rearrange('b e (h) (w) -> b (h w) e')\n",
        "        )\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
        "        self.position = nn.Parameter(torch.randn(\n",
        "            (img_size//patch_size)**2+1, emb_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, _, _, _ = x.shape\n",
        "        x = self.projection(x)\n",
        "        # cls token added  x batch times and appended\n",
        "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        x += self.position\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size: int = 768, num_heads: int = 12, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        # fuse the queries, keys and values in one matrix\n",
        "\n",
        "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
        "        self.att_drop = nn.Dropout(dropout)\n",
        "        self.projection = nn.Linear(emb_size, emb_size)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # split keys, queries and values in num_heads\n",
        "        # 3 x batch x no_head x sequence_length x emb_size\n",
        "        qkv = rearrange(\n",
        "            self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
        "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
        "        # sum up over the last axis\n",
        "        # batch, num_heads, query_len, key_len\n",
        "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
        "        if mask is not None:\n",
        "            fill_value = torch.finfo(torch.float32).min\n",
        "            energy.mask_fill(~mask, fill_value)\n",
        "\n",
        "        scaling = self.emb_size ** (1/2)\n",
        "        att = F.softmax(energy, dim=-1) / scaling\n",
        "        att = self.att_drop(att)\n",
        "        # sum up over the third axis\n",
        "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "        out = self.projection(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x\n",
        "\n",
        "\n",
        "class FeedForward(nn.Sequential):\n",
        "    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.1):\n",
        "        super().__init__(\n",
        "            nn.Linear(emb_size, expansion * emb_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_p),\n",
        "            nn.Linear(expansion * emb_size, emb_size),\n",
        "            nn.Dropout(drop_p),\n",
        "        )\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(nn.Sequential):\n",
        "    def __init__(self, emb_size=768, drop_p: float = 0.1, forward_expansion: int = 4, forward_drop_p: float = 0.1, ** kwargs):\n",
        "        super().__init__(\n",
        "            ResidualBlock(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                MultiHeadAttention(emb_size, **kwargs),\n",
        "                nn.Dropout(drop_p)\n",
        "            )),\n",
        "            ResidualBlock(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                FeedForward(emb_size),\n",
        "                nn.Dropout(drop_p)\n",
        "            )),\n",
        "        )\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Sequential):\n",
        "    def __init__(self, depth: int = 6, **kwargs):\n",
        "        super().__init__(*[TransformerEncoderBlock(**kwargs)\n",
        "                           for _ in range(depth)])\n",
        "\n",
        "\n",
        "class ViT(nn.Sequential):\n",
        "    def __init__(self,\n",
        "                 in_channels: int = 3,\n",
        "                 patch_size: int = 16,\n",
        "                 emb_size: int = 768,\n",
        "                 img_size: int = 256,\n",
        "                 depth: int = 6,\n",
        "                 **kwargs):\n",
        "        super().__init__(\n",
        "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
        "            TransformerEncoder(depth, emb_size=emb_size, **kwargs)\n",
        "        )\n",
        "\n",
        "\n",
        "# model = ViT()\n",
        "# print(model(torch.randn([1, 3, 32, 32])).shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from einops import rearrange\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class DecoderLinear(nn.Module):\n",
        "    def __init__(self, n_cls, patch_size, embedd_dim):\n",
        "        super().__init__()\n",
        "        self.n_cls = n_cls\n",
        "        self.patch_size = patch_size\n",
        "        self.embedd_dim = embedd_dim\n",
        "        self.head = nn.Linear(embedd_dim, n_cls)\n",
        "\n",
        "    def forward(self, x, img_size: int = 256):\n",
        "        H = W = img_size\n",
        "        num_patch = H//self.patch_size\n",
        "        x = self.head(x)\n",
        "        x = rearrange(x, \"b (h w) c -> b c h w\", h=num_patch)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MaskDecoder(nn.Module):\n",
        "    def __init__(self, scale, depth, patch_size, n_cls, dec_embdd):\n",
        "        super().__init__()\n",
        "        self.n_cls = n_cls\n",
        "        self.dec_embdd = dec_embdd\n",
        "        self.scale = scale\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        self.cls_emb = nn.Parameter(\n",
        "            torch.randn([1, n_cls, dec_embdd]))\n",
        "        self.dec_proj = nn.Linear(dec_embdd, dec_embdd)\n",
        "\n",
        "        self.proj_patch = nn.Parameter(\n",
        "            self.scale * torch.randn(dec_embdd, dec_embdd))\n",
        "        self.proj_classes = nn.Parameter(\n",
        "            self.scale * torch.randn(dec_embdd, dec_embdd))\n",
        "\n",
        "        self.decoder_norm = nn.LayerNorm(dec_embdd)\n",
        "        self.mask_norm = nn.LayerNorm(n_cls)\n",
        "\n",
        "        self.blocks = TransformerEncoder(depth=depth)\n",
        "\n",
        "    def forward(self, x, img_size):\n",
        "        H, W = img_size\n",
        "        GS = H//self.patch_size\n",
        "        x = self.dec_proj(x)\n",
        "        # Adding a cls token for each segmenting class\n",
        "        cls_emb = self.cls_emb.expand(x.size(0), -1, -1)\n",
        "        x = torch.cat((x, cls_emb), 1)\n",
        "        out = (self.blocks(x))\n",
        "        x = self.decoder_norm(out)\n",
        "\n",
        "        patches, cls_seg_feat = x[:, :-self.n_cls], x[:, -self.n_cls:]\n",
        "        patches = patches @ self.proj_patch  # 1 x 61 x 768\n",
        "        cls_seg_feat = cls_seg_feat @ self.proj_classes  # 1 x 4 x 768\n",
        "        patches = patches / patches.norm(dim=-1, keepdim=True)\n",
        "        cls_seg_feat = cls_seg_feat / cls_seg_feat.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        masks = patches @ cls_seg_feat.transpose(1, 2)\n",
        "        masks = self.mask_norm(masks)\n",
        "        masks = rearrange(masks, \"b (h w) n -> b n h w\", h=int(GS))\n",
        "        return masks"
      ],
      "metadata": {
        "id": "gq8aaSwrisSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import seaborn as sns\n",
        "import torch\n",
        "\n",
        "class Segmenter(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 scale,\n",
        "                 patch_size,\n",
        "                 image_size,\n",
        "                 enc_depth,\n",
        "                 dec_depth,\n",
        "                 enc_embdd,\n",
        "                 dec_embdd,\n",
        "                 n_cls):\n",
        "        super().__init__()\n",
        "        self.encoder = ViT(in_channels,\n",
        "                           patch_size,\n",
        "                           enc_embdd,\n",
        "                           image_size,\n",
        "                           enc_depth)\n",
        "        self.decoder = MaskDecoder(scale,\n",
        "                                   dec_depth,\n",
        "                                   patch_size,\n",
        "                                   n_cls,\n",
        "                                   dec_embdd)\n",
        "\n",
        "    def forward(self, img):\n",
        "        H, W = img.size(2), img.size(3)\n",
        "        x = self.encoder(img)\n",
        "        x = x[:, 1:]  # remove Cls token\n",
        "        masks = self.decoder(x, (H, W))\n",
        "        out = F.interpolate(masks, size=(H, W), mode=\"bilinear\")\n",
        "        return out\n",
        "\n",
        "model=Segmenter(3,0.05,16,256,12,6,768,768, 1)\n",
        "print(model(torch.randn([16,3,256,256])).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFjvGY5DkFpz",
        "outputId": "7ed48f28-821f-4d4f-b1c1-626e1a0bf257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 1, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qS_lzoYFaios"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from google.colab import drive\n",
        "import os  # Add this line\n",
        "\n",
        "# Define your dataset class\n",
        "drive.mount('/content/drive')\n",
        "# Custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, image_folder, mask_folder, transform=None):\n",
        "        self.root_folder_original = image_folder\n",
        "        self.root_folder_masked = mask_folder\n",
        "        self.transform = transform\n",
        "\n",
        "        self.original_images = os.listdir(image_folder)\n",
        "        self.masked_images = os.listdir(mask_folder)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.original_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        original_image_path = os.path.join(self.root_folder_original, self.original_images[idx])\n",
        "        masked_image_path = os.path.join(self.root_folder_masked, self.masked_images[idx])\n",
        "\n",
        "        original_image = Image.open(original_image_path).convert('RGB')\n",
        "        masked_image = Image.open(masked_image_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            original_image = self.transform(original_image)\n",
        "            masked_image = self.transform(masked_image)\n",
        "\n",
        "        return original_image, masked_image\n",
        "\n",
        "# Transformations for data augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Paths to your dataset folders\n",
        "image_folder_path = '/content/drive/My Drive/Project/Data_Full_Image/Original'\n",
        "mask_folder_path = '/content/drive/My Drive/Project/Data_Full_Image/Masked'\n",
        "\n",
        "# Create custom dataset\n",
        "dataset = CustomDataset(image_folder=image_folder_path, mask_folder=mask_folder_path, transform=transform)\n",
        "\n",
        "# Split dataset into train and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
        "\n",
        "# Initialize your model, optimizer, and loss function\n",
        "model = Segmenter(in_channels=3, scale=0.05, patch_size=16, image_size=256, enc_depth=12, dec_depth=6, enc_embdd=768, dec_embdd=768, n_cls=1)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.BCEWithLogitsLoss()  # Binary cross-entropy loss for binary segmentation\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, masks in train_loader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Assuming binary segmentation, convert masks to the same shape as outputs\n",
        "        masks = masks.squeeze(1).float()\n",
        "\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Validation loop\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, masks in val_loader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        sigmoid_outputs = torch.sigmoid(outputs)\n",
        "\n",
        "        # Convert to binary predictions (0 or 1) based on a threshold (e.g., 0.5)\n",
        "        preds = (sigmoid_outputs > 0.5).float()\n",
        "\n",
        "        # Flatten predictions and masks to 1D arrays\n",
        "        preds_flat = preds.view(-1)\n",
        "        masks_flat = masks.view(-1)\n",
        "\n",
        "        all_preds.append(preds_flat.cpu().numpy())\n",
        "        all_labels.append(masks_flat.cpu().numpy())\n",
        "\n",
        "    # Concatenate results from all batches\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds)\n",
        "    recall = recall_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRBcqO67DRH0",
        "outputId": "98c6bb7f-bf1b-4399-d45b-cbbbc2dcbff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ]
}